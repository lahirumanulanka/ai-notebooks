{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahirumanulanka/ai-notebooks/blob/main/03_optimize_nn_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAGcJgjpIO--"
      },
      "outputs": [],
      "source": [
        "# Sumani\n",
        "# https://www.linkedin.com/in/sumanaruban/\n",
        "# https://github.com/Sumanaruban\n",
        "# 29-7-2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcl3sU_zIO-_"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is a continuation of the previous Jupyter notebook [02_fully_connected_mnist.ipynb](./02_fully_connected_mnist.ipynb), where we built and trained a fully connected neural network to classify handwritten digits from the MNIST dataset using PyTorch. Having understood the fundamental steps involved in model development, this notebook provides a series of incremental exercises designed to deepen your understanding of various aspects of neural network training and optimization.\n",
        "\n",
        "Each exercise introduces a small modification to the existing model or training process. These modifications will help you explore the effects of different model architectures, activation functions, optimizers, learning rates, regularization techniques, data augmentation, loss functions, and evaluation metrics. By completing these exercises, you will gain hands-on experience in tuning neural networks and improving model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZSeBtRDIO-_"
      },
      "source": [
        "# How to Use This Notebook\n",
        "1. **Review the Previous Notebook**: Ensure you are familiar with the steps and code in the previous notebook, as this notebook builds upon that foundation.\n",
        "2. **Complete the Exercises**: Work through each exercise one by one. Make the specified changes to the model or training process and run the cells to see the effects.\n",
        "3. **Document Your Observations**: For each exercise, take note of how the changes impact the model's training and evaluation metrics. Provide explanations for any improvements or deteriorations in performance.\n",
        "4. **Experiment and Explore**: Feel free to experiment further by combining different techniques or exploring additional modifications beyond the provided exercises.\n",
        "\n",
        "By the end of this notebook, you will have a deeper understanding of how various factors influence the training and performance of neural networks, equipping you with the knowledge and skills to effectively tune and optimize models for real-world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyHk52APIO_A"
      },
      "source": [
        "## Exercise 1: Modify the Model Architecture\n",
        "\n",
        "    Add a Hidden Layer: Add an additional hidden layer with 256 neurons between the existing layers. Re-train the model and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "SDHRQy-WJmCV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "x_test = x_test.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build the model with an additional hidden layer\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dense(256, activation='relu'))  # Additional hidden layer with 256 neurons\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Save the model\n",
        "model.save('mnist_model_with_hidden_layer.h5')\n",
        "\n",
        "# Load the model (optional, for verification)\n",
        "loaded_model = keras.models.load_model('mnist_model_with_hidden_layer.h5')\n",
        "\n",
        "# Verify the loaded model\n",
        "loaded_test_loss, loaded_test_acc = loaded_model.evaluate(x_test, y_test)\n",
        "print(f'Loaded model test accuracy: {loaded_test_acc:.4f}')"
      ],
      "metadata": {
        "id": "m_82cXpQJ6qn",
        "outputId": "3f5e6efb-a3e4-4dc6-9900-e45c33b6e846",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8868 - loss: 0.3922 - val_accuracy: 0.9651 - val_loss: 0.1141\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9732 - loss: 0.0879 - val_accuracy: 0.9702 - val_loss: 0.0966\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9841 - loss: 0.0500 - val_accuracy: 0.9721 - val_loss: 0.0991\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9888 - loss: 0.0334 - val_accuracy: 0.9757 - val_loss: 0.0824\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9917 - loss: 0.0253 - val_accuracy: 0.9778 - val_loss: 0.0828\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9779\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9743 - loss: 0.0900\n",
            "Loaded model test accuracy: 0.9779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d82EqkjeIO_A"
      },
      "source": [
        "# Exercise 2: Change the Activation Function\n",
        "\n",
        "    Use Different Activation Functions: Replace the ReLU activation function with other activation functions such as Sigmoid or Tanh. Observe the impact on training and evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='tanh', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dense(256, activation='tanh'))  # Additional hidden layer with 256 neurons\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with Tanh activation\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model with Tanh activation\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy with Tanh activation: {test_acc:.4f}')\n",
        "\n",
        "# Save the model with Tanh activation\n",
        "model.save('mnist_model_with_tanh_activation.h5')\n",
        "\n",
        "# Load the model with Tanh activation (optional, for verification)\n",
        "loaded_model_tanh = keras.models.load_model('mnist_model_with_tanh_activation.h5')\n",
        "\n",
        "# Verify the loaded model with Tanh activation\n",
        "loaded_test_loss_tanh, loaded_test_acc_tanh = loaded_model_tanh.evaluate(x_test, y_test)\n",
        "print(f'Loaded model with Tanh activation test accuracy: {loaded_test_acc_tanh:.4f}')"
      ],
      "metadata": {
        "id": "IooOgdXYNP88",
        "outputId": "a6a72637-6a6a-4fe2-b6a2-59fa9b5f7d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8777 - loss: 0.4039 - val_accuracy: 0.9528 - val_loss: 0.1556\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9626 - loss: 0.1267 - val_accuracy: 0.9643 - val_loss: 0.1148\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9756 - loss: 0.0824 - val_accuracy: 0.9682 - val_loss: 0.1073\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9832 - loss: 0.0563 - val_accuracy: 0.9721 - val_loss: 0.0930\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9869 - loss: 0.0419 - val_accuracy: 0.9723 - val_loss: 0.0927\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9662 - loss: 0.1047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with Tanh activation: 0.9703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9662 - loss: 0.1047\n",
            "Loaded model with Tanh activation test accuracy: 0.9703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3TwnjfhIO_A"
      },
      "source": [
        "# Exercise 3: Change the Optimizer\n",
        "\n",
        "    Use Adam Optimizer: Replace the SGD optimizer with the Adam optimizer. Compare the training speed and final accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dense(256, activation='relu'))  # Additional hidden layer with 256 neurons\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model with SGD optimizer\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with SGD optimizer\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model with SGD optimizer\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy with SGD optimizer: {test_acc:.4f}')\n",
        "\n",
        "# Save the model with SGD optimizer\n",
        "model.save('mnist_model_with_sgd_optimizer.h5')\n",
        "\n",
        "# Load the model with SGD optimizer (optional, for verification)\n",
        "loaded_model_sgd = keras.models.load_model('mnist_model_with_sgd_optimizer.h5')\n",
        "\n",
        "# Verify the loaded model with SGD optimizer\n",
        "loaded_test_loss_sgd, loaded_test_acc_sgd = loaded_model_sgd.evaluate(x_test, y_test)\n",
        "print(f'Loaded model with SGD optimizer test accuracy: {loaded_test_acc_sgd:.4f}')"
      ],
      "metadata": {
        "id": "-yiVYpc_OAOX",
        "outputId": "49c4dfe4-625b-4b66-bc41-e9123b9e4a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.6552 - loss: 1.3636 - val_accuracy: 0.8938 - val_loss: 0.4041\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8923 - loss: 0.3981 - val_accuracy: 0.9127 - val_loss: 0.3111\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9107 - loss: 0.3144 - val_accuracy: 0.9237 - val_loss: 0.2729\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.9206 - loss: 0.2785 - val_accuracy: 0.9298 - val_loss: 0.2479\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9272 - loss: 0.2591 - val_accuracy: 0.9348 - val_loss: 0.2296\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9254 - loss: 0.2641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with SGD optimizer: 0.9350\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9254 - loss: 0.2641\n",
            "Loaded model with SGD optimizer test accuracy: 0.9350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0r-xJ7EIO_A"
      },
      "source": [
        "# Exercise 4: Adjust the Learning Rate\n",
        "\n",
        "    Experiment with Learning Rates: Try different learning rates (e.g., 0.1, 0.001, 0.0001) with the SGD and Adam optimizers. Record the effects on the model's training performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.1, 0.001, 0.0001]\n",
        "for lr in learning_rates:\n",
        "    print(f'\\nTraining with learning rate: {lr}')\n",
        "\n",
        "    # Compile the model with the current learning rate\n",
        "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=lr),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "    print(f'Test accuracy with SGD optimizer and learning rate {lr}: {test_acc:.4f}')\n",
        "\n",
        "    # Save the model with the current learning rate\n",
        "    model.save(f'mnist_model_with_sgd_lr_{lr}.h5')\n",
        "\n",
        "    # Load the model with the current learning rate (optional, for verification)\n",
        "    loaded_model_lr = keras.models.load_model(f'mnist_model_with_sgd_lr_{lr}.h5')\n",
        "    # Verify the loaded model with the current learning rate\n",
        "\n",
        "    loaded_test_loss_lr, loaded_test_acc_lr = loaded_model_lr.evaluate(x_test, y_test)\n",
        "    print(f'Loaded model with SGD optimizer and learning rate {lr} test accuracy: {loaded_test_acc_lr:.4f}')"
      ],
      "metadata": {
        "id": "Qftosmk_O6pL",
        "outputId": "4516255d-41f2-4746-de56-a93f2639efcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with learning rate: 0.1\n",
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9268 - loss: 0.2484 - val_accuracy: 0.9549 - val_loss: 0.1585\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9607 - loss: 0.1326 - val_accuracy: 0.9657 - val_loss: 0.1169\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9712 - loss: 0.0963 - val_accuracy: 0.9682 - val_loss: 0.1056\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9801 - loss: 0.0690 - val_accuracy: 0.9720 - val_loss: 0.0915\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9857 - loss: 0.0528 - val_accuracy: 0.9718 - val_loss: 0.0903\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9697 - loss: 0.0967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with SGD optimizer and learning rate 0.1: 0.9740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9697 - loss: 0.0967\n",
            "Loaded model with SGD optimizer and learning rate 0.1 test accuracy: 0.9740\n",
            "\n",
            "Training with learning rate: 0.001\n",
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9897 - loss: 0.0400 - val_accuracy: 0.9768 - val_loss: 0.0778\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9919 - loss: 0.0339 - val_accuracy: 0.9768 - val_loss: 0.0769\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9917 - loss: 0.0343 - val_accuracy: 0.9773 - val_loss: 0.0763\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9926 - loss: 0.0312 - val_accuracy: 0.9773 - val_loss: 0.0761\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9932 - loss: 0.0314 - val_accuracy: 0.9774 - val_loss: 0.0759\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.0809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with SGD optimizer and learning rate 0.001: 0.9782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.0809\n",
            "Loaded model with SGD optimizer and learning rate 0.001 test accuracy: 0.9782\n",
            "\n",
            "Training with learning rate: 0.0001\n",
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9929 - loss: 0.0310 - val_accuracy: 0.9774 - val_loss: 0.0759\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.9926 - loss: 0.0324 - val_accuracy: 0.9774 - val_loss: 0.0758\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9924 - loss: 0.0325 - val_accuracy: 0.9774 - val_loss: 0.0758\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9924 - loss: 0.0311 - val_accuracy: 0.9774 - val_loss: 0.0758\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9930 - loss: 0.0319 - val_accuracy: 0.9774 - val_loss: 0.0758\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9742 - loss: 0.0807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with SGD optimizer and learning rate 0.0001: 0.9781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9742 - loss: 0.0807\n",
            "Loaded model with SGD optimizer and learning rate 0.0001 test accuracy: 0.9781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-RItgtkIO_A"
      },
      "source": [
        "# Exercise 5: Implement Dropout\n",
        "\n",
        "    Add Dropout Layers: Introduce dropout layers with a dropout probability of 0.5 to the network. Check if the model's performance improves by reducing overfitting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "x_test = x_test.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build the model with dropout layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dropout(0.5))  # Dropout layer with 50% dropout rate\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))  # Another dropout layer with 50% dropout rate\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with dropout layers\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model with dropout layers\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy with dropout layers: {test_acc:.4f}')\n",
        "\n",
        "# Save the model with dropout layers\n",
        "model.save('mnist_model_with_dropout.h5')\n",
        "\n",
        "# Load the model with dropout layers (optional, for verification)\n",
        "loaded_model_dropout = keras.models.load_model('mnist_model_with_dropout.h5')\n",
        "\n",
        "# Verify the loaded model with dropout layers\n",
        "loaded_test_loss_dropout, loaded_test_acc_dropout = loaded_model_dropout.evaluate(x_test, y_test)\n",
        "print(f'Loaded model with dropout layers test accuracy: {loaded_test_acc_dropout:.4f}')"
      ],
      "metadata": {
        "id": "PLk0ceaeQKXT",
        "outputId": "b51cfce3-6969-40ae-b257-58a517037552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - accuracy: 0.7935 - loss: 0.6482 - val_accuracy: 0.9588 - val_loss: 0.1384\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9418 - loss: 0.1941 - val_accuracy: 0.9689 - val_loss: 0.1030\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9553 - loss: 0.1523 - val_accuracy: 0.9735 - val_loss: 0.0934\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9614 - loss: 0.1262 - val_accuracy: 0.9709 - val_loss: 0.0981\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9675 - loss: 0.1071 - val_accuracy: 0.9758 - val_loss: 0.0833\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.0885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with dropout layers: 0.9757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.0885\n",
            "Loaded model with dropout layers test accuracy: 0.9757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiac8YTBIO_A"
      },
      "source": [
        "# Exercise 6: Batch Normalization\n",
        "\n",
        "    Add Batch Normalization: Incorporate batch normalization layers into the network and observe any changes in training stability and speed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "x_test = x_test.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build the model with batch normalization layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.BatchNormalization())  # Batch normalization layer\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.BatchNormalization())  # Another batch normalization layer\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with batch normalization layers\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model with batch normalization layers\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy with batch normalization layers: {test_acc:.4f}')\n",
        "\n",
        "# Save the model with batch normalization layers\n",
        "model.save('mnist_model_with_batch_normalization.h5')\n",
        "\n",
        "# Load the model with batch normalization layers (optional, for verification)\n",
        "loaded_model_batch_norm = keras.models.load_model('mnist_model_with_batch_normalization.h5')\n",
        "# Verify the loaded model with batch normalization layers\n",
        "\n",
        "loaded_test_loss_batch_norm, loaded_test_acc_batch_norm = loaded_model_batch_norm.evaluate(x_test, y_test)\n",
        "print(f'Loaded model with batch normalization layers test accuracy: {loaded_test_acc_batch_norm:.4f}')"
      ],
      "metadata": {
        "id": "zegkKzMt5Kfo",
        "outputId": "cceea915-cf5e-44b7-f8cf-2f5937114124",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.9039 - loss: 0.3118 - val_accuracy: 0.9682 - val_loss: 0.1071\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9737 - loss: 0.0856 - val_accuracy: 0.9707 - val_loss: 0.0951\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9809 - loss: 0.0597 - val_accuracy: 0.9720 - val_loss: 0.0953\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9857 - loss: 0.0424 - val_accuracy: 0.9728 - val_loss: 0.1052\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9894 - loss: 0.0337 - val_accuracy: 0.9723 - val_loss: 0.0973\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.1060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with batch normalization layers: 0.9749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9695 - loss: 0.1060\n",
            "Loaded model with batch normalization layers test accuracy: 0.9749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjTfKvcJIO_A"
      },
      "source": [
        "# Exercise 7: Data Augmentation\n",
        "\n",
        "    Apply Data Augmentation: Implement data augmentation techniques like random rotations, shifts, and flips to the training dataset. Evaluate the model's robustness to these transformations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,  # Random rotations\n",
        "    width_shift_range=0.1,  # Random horizontal shifts\n",
        "    height_shift_range=0.1,  # Random vertical shifts\n",
        "    horizontal_flip=False,  # MNIST is not typically flipped horizontally\n",
        "    zoom_range=0.1  # Random zoom\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the data generator\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "          epochs=5,\n",
        "          validation_data=(x_test, y_test),\n",
        "          steps_per_epoch=len(x_train) // 64)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy with data augmentation: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "0v7_LhH-8Fzb",
        "outputId": "5c3b3ec1-ebee-48ba-a109-8ad3cbef2229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 73ms/step - accuracy: 0.8026 - loss: 0.6142 - val_accuracy: 0.9843 - val_loss: 0.0494\n",
            "Epoch 2/5\n",
            "\u001b[1m  1/937\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 46ms/step - accuracy: 0.9531 - loss: 0.1369"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9531 - loss: 0.1369 - val_accuracy: 0.9841 - val_loss: 0.0487\n",
            "Epoch 3/5\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 72ms/step - accuracy: 0.9641 - loss: 0.1176 - val_accuracy: 0.9845 - val_loss: 0.0475\n",
            "Epoch 4/5\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0223 - val_accuracy: 0.9854 - val_loss: 0.0473\n",
            "Epoch 5/5\n",
            "\u001b[1m937/937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 74ms/step - accuracy: 0.9743 - loss: 0.0814 - val_accuracy: 0.9877 - val_loss: 0.0361\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9839 - loss: 0.0453\n",
            "Test accuracy with data augmentation: 0.9877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyaQ4k5IIO_A"
      },
      "source": [
        "# Exercise 8: Learning Rate Scheduling\n",
        "\n",
        "    Implement Learning Rate Scheduling: Use a learning rate scheduler to decrease the learning rate during training. Compare the training process and model performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "x_test = x_test.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dense(256, activation='relu'))  # Additional hidden layer with 256 neurons\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define a learning rate scheduler\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))  # Ensure the returned value is a Python float\n",
        "\n",
        "# Create a LearningRateScheduler callback\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Train the model with learning rate scheduling\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[lr_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy with learning rate scheduling: {test_acc:.4f}')\n",
        "\n",
        "# Save the model with learning rate scheduling\n",
        "model.save('mnist_model_with_lr_scheduling.h5')\n",
        "\n",
        "# Load the model with learning rate scheduling (optional, for verification)\n",
        "loaded_model_lr_sched = keras.models.load_model('mnist_model_with_lr_scheduling.h5')\n",
        "\n",
        "# Verify the loaded model with learning rate scheduling\n",
        "loaded_test_loss_lr_sched, loaded_test_acc_lr_sched = loaded_model_lr_sched.evaluate(x_test, y_test)\n",
        "print(f'Loaded model with learning rate scheduling test accuracy: {loaded_test_acc_lr_sched:.4f}')"
      ],
      "metadata": {
        "id": "_7Ui07Rf-C2i",
        "outputId": "31b1622d-9272-4099-9d44-0c4b6a9e8bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.8864 - loss: 0.3880 - val_accuracy: 0.9586 - val_loss: 0.1290 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9728 - loss: 0.0871 - val_accuracy: 0.9703 - val_loss: 0.0997 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9834 - loss: 0.0523 - val_accuracy: 0.9692 - val_loss: 0.0993 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9881 - loss: 0.0372 - val_accuracy: 0.9758 - val_loss: 0.0832 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9923 - loss: 0.0236 - val_accuracy: 0.9746 - val_loss: 0.0951 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9932 - loss: 0.0204 - val_accuracy: 0.9781 - val_loss: 0.0871 - learning_rate: 9.0484e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9967 - loss: 0.0109 - val_accuracy: 0.9767 - val_loss: 0.1035 - learning_rate: 8.1873e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9964 - loss: 0.0101 - val_accuracy: 0.9787 - val_loss: 0.0947 - learning_rate: 7.4082e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9986 - loss: 0.0056 - val_accuracy: 0.9816 - val_loss: 0.0917 - learning_rate: 6.7032e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 0.9812 - val_loss: 0.0935 - learning_rate: 6.0653e-04\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9801 - loss: 0.0960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with learning rate scheduling: 0.9841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9801 - loss: 0.0960\n",
            "Loaded model with learning rate scheduling test accuracy: 0.9841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3sCXW-IIO_A"
      },
      "source": [
        "# Exercise 9: Change the Loss Function\n",
        "\n",
        "    Use Different Loss Functions: Experiment with different loss functions such as Mean Squared Error (MSE) and observe how the choice of loss function affects model training and performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "x_test = x_test.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dense(256, activation='relu'))  # Additional hidden layer with 256 neurons\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Train the model with categorical crossentropy\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "# Evaluate the model with categorical crossentropy\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy with categorical crossentropy: {test_acc:.4f}')\n",
        "# Change the loss function to Mean Squared Error (MSE)\n",
        "model.compile(optimizer='adam',\n",
        "                loss='mean_squared_error',  # Change to MSE\n",
        "                metrics=['accuracy'])\n",
        "# Train the model with MSE loss function\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "# Evaluate the model with MSE loss function\n",
        "test_loss_mse, test_acc_mse = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy with Mean Squared Error loss: {test_acc_mse:.4f}')\n",
        "# Save the model with MSE loss function\n",
        "model.save('mnist_model_with_mse_loss.h5')\n",
        "# Load the model with MSE loss function (optional, for verification)\n",
        "loaded_model_mse = keras.models.load_model('mnist_model_with_mse_loss.h5')\n",
        "# Verify the loaded model with MSE loss function\n",
        "loaded_test_loss_mse, loaded_test_acc_mse = loaded_model_mse.evaluate(x_test, y_test)\n",
        "print(f'Loaded model with MSE loss test accuracy: {loaded_test_acc_mse:.4f}')"
      ],
      "metadata": {
        "id": "lU3gq6fE_ATn",
        "outputId": "1725b9d7-e63f-4645-98bd-3d7de0474d5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8820 - loss: 0.3873 - val_accuracy: 0.9663 - val_loss: 0.1100\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9748 - loss: 0.0828 - val_accuracy: 0.9711 - val_loss: 0.0979\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9835 - loss: 0.0520 - val_accuracy: 0.9732 - val_loss: 0.0948\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9879 - loss: 0.0377 - val_accuracy: 0.9754 - val_loss: 0.0848\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9922 - loss: 0.0249 - val_accuracy: 0.9776 - val_loss: 0.0800\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9751 - loss: 0.0846\n",
            "Test accuracy with categorical crossentropy: 0.9791\n",
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9892 - loss: 0.0016 - val_accuracy: 0.9744 - val_loss: 0.0040\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9906 - loss: 0.0014 - val_accuracy: 0.9725 - val_loss: 0.0044\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9917 - loss: 0.0013 - val_accuracy: 0.9778 - val_loss: 0.0035\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9935 - loss: 0.0011 - val_accuracy: 0.9750 - val_loss: 0.0041\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9934 - loss: 0.0010 - val_accuracy: 0.9711 - val_loss: 0.0047\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9634 - loss: 0.0060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with Mean Squared Error loss: 0.9691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9634 - loss: 0.0060\n",
            "Loaded model with MSE loss test accuracy: 0.9691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXOsTfBdIO_A"
      },
      "source": [
        "# Exercise 10: Evaluate with Different Metrics\n",
        "\n",
        "    Implement Precision, Recall, and F1-Score: Extend the evaluation metrics to include precision, recall, and F1-score. Analyze the model's performance using these metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "x_test = x_test.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dense(256, activation='relu'))  # Additional hidden layer with 256 neurons\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Get predictions\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Convert predictions and true labels to class indices\n",
        "y_pred_classes = tf.argmax(y_pred, axis=1).numpy()\n",
        "y_true_classes = tf.argmax(y_test, axis=1).numpy()\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')\n",
        "\n",
        "# Save the model\n",
        "model.save('mnist_model_with_metrics.h5')\n",
        "\n",
        "# Load the model (optional, for verification)\n",
        "loaded_model_metrics = keras.models.load_model('mnist_model_with_metrics.h5')\n",
        "\n",
        "# Verify the loaded model\n",
        "loaded_test_loss_metrics, loaded_test_acc_metrics = loaded_model_metrics.evaluate(x_test, y_test)\n",
        "print(f'Loaded model test accuracy: {loaded_test_acc_metrics:.4f}')"
      ],
      "metadata": {
        "id": "kuPLzwql_vEN",
        "outputId": "fc935762-3d92-494b-9d69-896d8219b50d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8837 - loss: 0.3939 - val_accuracy: 0.9650 - val_loss: 0.1183\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9727 - loss: 0.0855 - val_accuracy: 0.9757 - val_loss: 0.0869\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9833 - loss: 0.0515 - val_accuracy: 0.9732 - val_loss: 0.0900\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.9886 - loss: 0.0351 - val_accuracy: 0.9766 - val_loss: 0.0821\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.9913 - loss: 0.0258 - val_accuracy: 0.9745 - val_loss: 0.1027\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9730 - loss: 0.0977\n",
            "Test accuracy: 0.9769\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9770\n",
            "Recall: 0.9769\n",
            "F1-Score: 0.9769\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.0977\n",
            "Loaded model test accuracy: 0.9769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFLQBbg5IO_A"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "These exercises will help you to understand the impact of different neural network architectures, optimization techniques, and evaluation metrics on the model's performance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}