{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93313539",
      "metadata": {
        "id": "93313539"
      },
      "source": [
        "# CIFAR-10 Classification using Fully Connected Neural Network (FCN)\n",
        "\n",
        "This notebook implements a Fully Connected Neural Network (FCN) using PyTorch to classify images from the CIFAR-10 dataset. This is intended to be a baseline for comparison with a Convolutional Neural Network (CNN) model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "686c4ad3",
      "metadata": {
        "id": "686c4ad3",
        "outputId": "a1cc598c-68bb-43f2-86ff-b02845b0eab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f70586",
      "metadata": {
        "id": "46f70586"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dbae80da",
      "metadata": {
        "id": "dbae80da",
        "outputId": "f5128d31-91e7-497d-95ae-a3cc95c75016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42487bd6",
      "metadata": {
        "id": "42487bd6"
      },
      "source": [
        "## Define the Fully Connected Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1bc2e705",
      "metadata": {
        "id": "1bc2e705"
      },
      "outputs": [],
      "source": [
        "class FCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCN, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)  # Flatten the image\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "net = FCN().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b092a512",
      "metadata": {
        "id": "b092a512"
      },
      "source": [
        "## Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "77bf3414",
      "metadata": {
        "id": "77bf3414"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3b70aa",
      "metadata": {
        "id": "0f3b70aa"
      },
      "source": [
        "## Train the Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "79e2a66a",
      "metadata": {
        "id": "79e2a66a",
        "outputId": "bb01bc5e-e2fd-4d48-b140-ff87fed20d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 100] loss: 1.862\n",
            "[1, 200] loss: 1.686\n",
            "[1, 300] loss: 1.635\n",
            "[1, 400] loss: 1.570\n",
            "[1, 500] loss: 1.545\n",
            "Epoch 1 Test Accuracy: 46.55%\n",
            "[2, 100] loss: 1.448\n",
            "[2, 200] loss: 1.447\n",
            "[2, 300] loss: 1.435\n",
            "[2, 400] loss: 1.431\n",
            "[2, 500] loss: 1.424\n",
            "Epoch 2 Test Accuracy: 50.27%\n",
            "[3, 100] loss: 1.300\n",
            "[3, 200] loss: 1.302\n",
            "[3, 300] loss: 1.308\n",
            "[3, 400] loss: 1.321\n",
            "[3, 500] loss: 1.321\n",
            "Epoch 3 Test Accuracy: 51.40%\n",
            "[4, 100] loss: 1.192\n",
            "[4, 200] loss: 1.209\n",
            "[4, 300] loss: 1.242\n",
            "[4, 400] loss: 1.218\n",
            "[4, 500] loss: 1.210\n",
            "Epoch 4 Test Accuracy: 52.15%\n",
            "[5, 100] loss: 1.107\n",
            "[5, 200] loss: 1.098\n",
            "[5, 300] loss: 1.121\n",
            "[5, 400] loss: 1.125\n",
            "[5, 500] loss: 1.143\n",
            "Epoch 5 Test Accuracy: 53.69%\n",
            "[6, 100] loss: 1.006\n",
            "[6, 200] loss: 1.024\n",
            "[6, 300] loss: 1.027\n",
            "[6, 400] loss: 1.043\n",
            "[6, 500] loss: 1.065\n",
            "Epoch 6 Test Accuracy: 53.33%\n",
            "[7, 100] loss: 0.919\n",
            "[7, 200] loss: 0.930\n",
            "[7, 300] loss: 0.940\n",
            "[7, 400] loss: 0.959\n",
            "[7, 500] loss: 0.973\n",
            "Epoch 7 Test Accuracy: 54.15%\n",
            "[8, 100] loss: 0.828\n",
            "[8, 200] loss: 0.844\n",
            "[8, 300] loss: 0.862\n",
            "[8, 400] loss: 0.897\n",
            "[8, 500] loss: 0.902\n",
            "Epoch 8 Test Accuracy: 54.68%\n",
            "[9, 100] loss: 0.713\n",
            "[9, 200] loss: 0.768\n",
            "[9, 300] loss: 0.785\n",
            "[9, 400] loss: 0.815\n",
            "[9, 500] loss: 0.838\n",
            "Epoch 9 Test Accuracy: 54.29%\n",
            "[10, 100] loss: 0.673\n",
            "[10, 200] loss: 0.686\n",
            "[10, 300] loss: 0.711\n",
            "[10, 400] loss: 0.739\n",
            "[10, 500] loss: 0.743\n",
            "Epoch 10 Test Accuracy: 54.14%\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "writer = SummaryWriter()\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
        "            writer.add_scalar(\"training loss\", running_loss / 100, epoch * len(trainloader) + i)\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Evaluate on test data after each epoch\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for test_data in testloader:\n",
        "            test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
        "            test_outputs = net(test_images)\n",
        "            _, predicted = torch.max(test_outputs.data, 1)\n",
        "            total += test_labels.size(0)\n",
        "            correct += (predicted == test_labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1} Test Accuracy: {accuracy:.2f}%\")\n",
        "    writer.add_scalar(\"Test Accuracy\", accuracy, epoch)\n",
        "writer.close()\n",
        "print(\"Finished Training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "110881db",
      "metadata": {
        "id": "110881db"
      },
      "source": [
        "## Evaluate on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1cbdc4c3",
      "metadata": {
        "id": "1cbdc4c3",
        "outputId": "9df766fa-6f93-40b4-988e-abc93c1e05b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 54.14%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deeplearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}